# -*- coding: utf-8 -*-
"""dicoding_submission_predictive_analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJ9Z7KuW2NQBPX4bGVWKxBqe_-YBO1sk
"""

!pip install missingno

# Library

import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import missingno
from IPython.display import display

# Preprocessing
from sklearn.preprocessing import OneHotEncoder,LabelEncoder, StandardScaler, RobustScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE
from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks

# Model
import multiprocessing
import time
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

# Model Evaluation
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, RocCurveDisplay, PrecisionRecallDisplay, recall_score, f1_score, roc_auc_score, accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Set max columns
pd.set_option('display.max_columns', None)

# Warning
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""# E-commerce Customer Churn: Predicting Customer Churn

Created By Christianto Kurniawan Priyono

# Business Understanding

## Project Background

Fenomena churn pelanggan dalam industri e-commerce, yang mencerminkan kehilangan pelanggan setelah transaksi atau pengalaman negatif, telah menjadi permasalahan serius yang mempengaruhi perusahaan di seluruh dunia. Dampaknya terlihat melalui penuruan pendapatan yang signifikan, dan penelitian menunjukkan bahwa peningkatan retensi pelanggan sebesar 5% dapat menghasilkan kenaikan keuntungan yang substansial.<br>
Persaingan yang sengit di pasar global e-commerce memperkuat pentingnya menjaga pelanggan dengan fokus pada peningkatan pengalaman dan retensi. Kualitas layanan, harga kompetitif, dan kebijakan pengembalian yang efisien menjadi faktor utama, sementara perubahan preferensi konsumen dan ekspektasi yang tinggi menambahkan kompleksitas. <br>
Untuk mengatasi tantangan ini, perusahaan e-commerce semakin mengandalkan teknologi seperti machine learning untuk membangun model prediksi yang mengidentifikasi pelanggan beresiko churn, memungkinkan mereka mengembangkan strategi retensi yang efektif berdasarkan data pelanggan historis, transaksi, dan umpan balik *(feedback)*. Pemahaman yang mendalam tentang churn dan respons proaktif menjadi kunci bagi perusahaan e-commerce untuk menjaga keberlanjutan dan kompetitivitas di pasar yang terus berkembang.

## Problem Statement

Dalam sebuah perusahaan, tingkat churn yang tinggi adalah indikator kegagalan dalam menjaga pelanggan dalam bisnis E-Commerce. Ketika churn rate mencapai 5% artinya perusahaan akan kehilangan 5% dari pelanggan mereka. Oleh karena itu, sangat penting bagi perusahaan untuk berusaha mengurangi churn rate ini sekecil mungkin. Karena, mencari dan mendapatkan pelanggan baru memerlukan investasi yang lebih besar dibandingkan dengan menjaga pelanggan yang sudah ada. <br>
Sebagai contoh, apabila perusahaan menghabiskan 100 dollars perbulan untuk mendapatkan pelanggan baru dan biaya per pelanggan per bulan sebesar 50 dollars, maka perusahaan harus memastikan setidaknya 2 pelanggan yang tetap bertahan setiap bulannya untuk menutupi biaya tersebut. [Sumber](https://www.woopra.com/blog/churn-rate-vs-retention-rate)
<br>

Berdasarkan project background diatas, Bagaimana perusahaan dapat secara efektif mengidentifikasi pelanggan yang beresiko churn dengan menggunakan data historis, transaksi dan umpan balik pelanggan?

## Goals

Dalam rangka mengatasi permasalahan *churn* pelanggan. Perusahaan memiliki tujuan yaitu mengurangi tingkat churn pelanggan dalam bisnis *e-commerce* menjadi sekecil mungkin, dengan fokus pada menciptakan strategi retensi yang efektif.

## Solution Statement

Untuk mengatasi permasalahan churn pelanggan dalam bisnis e-commerce dan mencapai tujuan yang telah ditetapkan. Berikut langkah-langkah dalam solution statement:
* Pengumpulan data yang relevan
  <br> Mengumpulkan dan mengintegrasikan data yang relevan dan berkualitas tinggi untuk digunakan sebagai dasar untuk analisis dan prediksi churn
* Pengembangan model machine learning.
  <br> Menggunakan algoritma machine learning seperti Regresi Logistik, Random Forest dll untuk mengembangkan model prediksi churn pada pelanggan masa depan.

## Metric Evaluation

Target utama dalam masalah ini adalah pelanggan yang berhenti berlangganan (Churn).

<br>

**False Positive:** Pelanggan yang aktualnya tidak tapi diprediksi churn
<br> **Konsekuensi**: Tidak efektif dalam pemberian insentif

**False Negative:** Pelanggan yang aktualnya churn tetapi diprediksi tidak akan churn
<br> **Konsekuensi:** Kehilangan Pelanggan.


<br>
Berdasarkan konsekuensi yang ada, akan dibuat model machine learning yang akan mengurangi resiko kehilangan pelanggan karena untuk mendapatkan pelanggan baru membutuhkan biaya lebih banyak. Kita ingin recall dan precision seimbang sehingga digunakan metric f1_score

# Data Understanding

## Attribute Information

Dataset Source: https://www.kaggle.com/datasets/ankitverma2010/ecommerce-customer-churn-analysis-and-prediction

| Attribute        | Data Type        | Description  |
|------------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
CustomerID              | Int   | Unique customer ID	|
Churn                   | Int | Churn Flag 0 : Tidak Churn, 1 : Churn |
Tenure                  | Float | Tenure of customer in organization |
PreferredLoginDevice    | Text | Preferred login device of customer |
CityTier                | Int | City tier |
WarehouseToHome         | Float | Distance in between warehouse to home of customer	 |
PreferredPaymentMode    | Text | Preferred payment method of customery |
Gender                  | Text    | Gender of Customer |
HourSpendOnApp          | Float | Number of hours spend on mobile application or website |
NumberOfDeviceRegistered | Int | Total number of added added on particular customer  |
PreferedOrderCat   | Object | Preferred order category of customer in last month |
SatisfactionScore   | Int | Satisfactory score of customer on service |
MaritalStatus   | Object | Marital status of customer |
NumberOfAddress   | Int | Total number of added added on particular customer |
Complain   | Int | 	Any complaint has been raised in last month |
OrderAmountHikeFromlastYear  | Float | Percentage increases in order from last year |
CouponUsed   | Float | Total number of coupon has been used in last month |
OrderCount   | Float | Total number of orders has been places in last month |
DaySinceLastOrder | Float | Day Since last order by customer |
CashbackAmount | Float | Average cashback in last month |
"""

# Load Dataset
df = pd.read_excel('/content/drive/MyDrive/Portfolio/machine_learning/dicoding_submisson_predictive_analytics/E-Commerce Dataset.xlsx', sheet_name='E Comm')
df.head()

# Mengetahui info tentang dataset.
df.info()

"""Dari data diatas, terlihat bahwa terdapat 5630 baris dan 20 kolom

"""

# Check description dataset
df.describe()

"""Terdapat missing kolom pada dataset karena pada kolom "min" terdapat nilai 0.

# Exploratory Data Analysis

## Checking Unique Values (EDA)

Tujuan dari langkah ini adalah untuk mengidentifikasi dan memahami variasi nilai yang ada dalam satu set data.
"""

# Checking Value Dataset
listItem = []
for col in df.columns:
    listItem.append([col, df[col].dtype, df[col].isna().sum(), round((df[col].isna().sum() / len(df[col])) * 100, 2),
                     df[col].nunique(), list(df[col].unique())])

dfDesc = pd.DataFrame(columns=['dataFeatures', 'dataType', 'null', 'nullPct', 'unique', 'uniqueSample'], data=listItem)
dfDesc

"""Dari data diatas, dataset mempunyai 264 missing value pada kolom `tenure`, 251 missing value pada kolom `WarehouseToHome`, 255 missing value pada kolom `HourSpendOnApp`, 265 pada kolom `OrderAmountHikeFromlastYear`, 256 pada kolom `CouponUsed`, 258 pada kolom `OrderCount`, dan 307 pada kolom `DaySinceLastOrder`.

**PreferedOrderCat**

Kemudian kita check unique values pada kolom PrederedOrderCat, jika mempunyai value yang sama maka akan kita sederhanakan.
"""

df['PreferedOrderCat'].unique()

"""Terdapat value yang sama antara Mobile Phone dengan Mobile, maka akan kita jadikan satu menjadi Mobile Phone"""

df['PreferedOrderCat'] = df['PreferedOrderCat'].replace({'Mobile':'Mobile Phone'})
df['PreferedOrderCat'].unique()

"""**PreferredPaymentMode**

Kemudian kita check unique values pada kolom PreferredPaymentMode, jika mempunyai value yang sama maka akan kita sederhanakan.
"""

df['PreferredPaymentMode'].unique()

"""Terdapat value yang sama antara CC dengan Credit Card dan COD dengan Cash on Delivery, maka akan kita jadikan satu CC menjadi Credit Card dan COD menjadi Cash on Delivery"""

df['PreferredPaymentMode'] = df['PreferredPaymentMode'].replace({'CC':'Credit Card'})
df['PreferredPaymentMode'] = df['PreferredPaymentMode'].replace({'COD':'Cash on Delivery'})
df['PreferredPaymentMode'].unique()

"""**PreferredLoginDevice**

Kemudian kita check unique values pada kolom PreferredPaymentMode, jika mempunyai value yang sama maka akan kita sederhanakan.
"""

df['PreferredLoginDevice'].unique()

"""Terdapat value yang sama antara Mobile Phone dengan Phone, maka akan kita jadikan satu menjadi Mobile Phone"""

df['PreferredLoginDevice'] = df['PreferredLoginDevice'].replace({'Phone':'Mobile Phone'})
df['PreferredLoginDevice'].unique()

"""## Checking Correlation"""

corr = df.corr('spearman') # menggunakan spearman karena tidak check distribusi normal atau tidak.
plt.figure(figsize=(10,5))
matrix = np.triu(corr)
sns.heatmap(corr, annot=True, fmt='.3f', mask=matrix, cmap='RdYlGn')
plt.title('Correlation Feature vs Churn (Target)', size=14, weight='bold');

"""## Handling

### Handling Columns

Karena Column Customer_ID tidak digunakan, maka akan kita hapus saja
"""

# menghapus kolom CustomerID
df.drop(columns='CustomerID', inplace=True)

# check info dataset
df.info()

"""### Handling Duplicate Value

Terdapat beberapa tujuan dari proses ini, yaitu:
* Memastikan kualitas data <br>
  Duplikasi data dapat menyebabkan bias dan mengganggu hasil analisis yang akurat.
* Menghindari Perkiraan Berlebihan <br>
  Jika data ini tidak ditangani, perkiraan atau statistik yang didasarkan pada data tersebut dapat menjadi terlalu berlebihan. Ini dapat menghasilkan evaluasi yang tidak akurat dalam berbagai konteks.
"""

# Number of Dulicate Data
df.duplicated().sum()

# Percentage of Duplicate Data
duplicate_percentage = (df.duplicated().sum() / len(df)) * 100
duplicate_percentage

"""Pada dataset kita terdapat 556 data duplicate atau 9.87%. Maka langkah selanjutnya kita akan menghapus data duplicate tersebut."""

# Handling Dupliacate by Deleting
df.drop_duplicates(inplace=True)

# Checking Duplicate After Handling
df.duplicated().sum()

"""### Handling Missing Value

Tujuan dari mengatasi nilai yang hilang adalah sebagai berikut:
* Memastikan Kualitas Data <br>
  Data yang hilang dapat menyebabkan bias dan menganggu hasil analisis yang akurat
* Menghindari Kesalahan Analisis <br>
  Nilai yang hilang dapat mengakibatkan kesalahan dalam analisis data, seperti perhitungan yang salah atau penarikan kesimpulan yang tidak tepat.
"""

missingno.bar(df, sort="descending", figsize=(12,6), fontsize=12, color='blue')

# Check missing value
df.isna().sum()/len(df)*100

"""Missing value pada data hanya sebesar 4.5% - 5.4% dari total keseluruhan data. Sehingga data ini akan di delete (karena menurut beberapa sumber, maksimal penghapusan data hanya 10% dari total dataset)."""

# Hapus missing kolom
df.dropna(inplace= True)

# Check missing value
df.isna().sum()/len(df)*100

"""### Handling Outliers

Pertama kita check outliers yang terdapat pada feature numerik.
"""

num_feature = [fea for fea in df.describe().columns if fea not in ['Churn', 'Complain']]
num_feature

plt.figure(figsize=(15, 9), facecolor='white')
plotnumber = 1

for feature in num_feature:
    ax = plt.subplot(4,4, plotnumber)
    sns.boxplot(x=feature, data=df);
    plt.title(feature, fontsize=12)
    plt.xlabel(None)
    plt.tight_layout()
    plotnumber += 1

"""Akan dilakukan handling outlier menggunakan metode IQR *(Inter Quartile Range)*"""

def detect_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (df < lower_bound) | (df > upper_bound)

# delete outliers in numerical feature kecuali churn dan complain
for feature in num_feature:
    if feature not in ['Churn', 'Complain']:
        df = df[~(detect_outliers(df[feature]))]

df.shape

"""## Unvariate Analysis"""

# membagi dataset menjadi dua bagian
cat_features = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus']

"""### Categorical Features

Fitur **PreferredLoginDevice**
"""

feature = cat_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_visualization = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df_visualization)
count.plot(kind='bar', title=feature);

"""Dari tabel diatas, dapat kita ketahui bahwa mayoritas pelanggan, sekitar 71.4% lebih memilih "Mobile Phone" sebagai perangkat utama mereka untuk login, sementara 28.6% lainnya lebih cenderung menggunakan "Computer" untuk perangkat preferensi. Temuan ini mengindikasikan dominasi penggunaan perangkat mobile dalam mengakses platform e-commerce.<br>

Tren ini dapat menjadi pedoman berharga bagi perusahaan e-commerce dalam merancang strategi bisnis. Mereka dapat memprioritaskan pengoptimalan pelanggan di perangkat mobile. Dengan demikian, mereka dapat meningkatkan kepuasan pelanggan yang lebih cenderung menggunakan perangkat mobile untuk belanja online. Data ini juga memberikan peluang untuk mengarahkan strategi pemasaran yang lebih terforkus. <br>

Namun, penting juga untuk terus memantau dan menganalisis tren ini seiring berjalannyua waktu karena preferensi pelanggan dapat berubah.
"""

feature = cat_features[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_visualization = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df_visualization)
count.plot(kind='bar', title=feature);

"""Metode pembayaran yang paling dominan adalah "Debit Card" sekitar 41.1% dari total pelanggan memilih metode ini. "Credit Card" juga cukup populer, digunakan oleh sekitar 30.1% pelanggan. Sementara itu, metode pembayaran lain seperti "E-Wallet", "Cash on Delivery", dan "UPI" digunakan oleh persentase pelanggan yang lebih kecil, masing-masing sekitar 12.1%, 9.0%, 7.7% <br>

Metode pembayaran non-tunai seperti kartu debit, kredit dan E-Wallet lebih disukai daripada pembayaran tunai (Cash on Delivery). Perusahaan e-commerce dapat menggunakan informasi ini untuk menyusun strategi pembayaran yang lebih efisien, termasuk penawaran promosi khusus untuk pengguna kartu debit atau kredit. <br>

Namun, perlu juga diperhatikan bahwa metode pembayaran tunai seperti "Cash on Delivery" masih digunakan oleh sebagian pelanggan. Oleh karena itu, perusahaan perlu menjaga fleksibilitas dalam penawaran metode pembayaran dan memastikan bahwa pelanggan yang menggunakan metode ini juga mendapatkan pengalaman yang baik dan aman saat berbelanja online.
"""

feature = cat_features[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_visualization = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df_visualization)
count.plot(kind='bar', title=feature);

"""Terlihat bahwa mayoritas pelanggan, sekitar 60.7%, adalah pelanggan dengan jenis kelamin "Male" (pria), sementara sekitar 39.3% lainnya adalah pelanggan dengan jenis kelamin "Female" (wanita)."""

feature = cat_features[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_visualization = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df_visualization)
count.plot(kind='bar', title=feature);

"""Terlihat bahwa mayoritas pelanggan, sekitar 50.3%, lebih memilih untuk berbelanja dalam kategori "Laptop & Accessory" (laptop dan aksesorinya). Sementara itu, sekitar 41.1% pelanggan lebih suka berbelanja dalam kategori "Mobile Phone" (telepon seluler), dan sekitar 8.7% lainnya memilih kategori "Fashion" (mode)."""

feature = cat_features[4]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_visualization = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df_visualization)
count.plot(kind='bar', title=feature);

"""Terlihat bahwa mayoritas pelanggan, sekitar 49.9%, adalah pelanggan yang berstatus "Married" (menikah). Kemudian sekitar 34.0% pelanggan adalah "Single" (single/tidak menikah), dan sekitar 16.1% pelanggan merupakan pelanggan yang "Divorced" (bercerai).

### Numerical Features
"""

# histogram masing-masing fitur pada dataset
df.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari visualisasi diatas, dapat dilihat bahwa rata-rata distribusi pada kolom numerical yaitu *right-skewed* (miring ke kanan)

## Checking Data Proportion
"""

# Check data proportion using value_counts
df['Churn'].value_counts()

# Visualization
plt.figure(figsize=(7,7))
plt.title("Number of Targets(Churn) in the dataset", size=18, weight='bold')
plt.pie(df['Churn'].value_counts(), explode=(0,0.1), labels=['No', 'Yes'], colors=["orange","green"], autopct='%1.1f%%', shadow=True,
        startangle=90, textprops={"fontsize":20})

plt.show()

"""Dari visualisasi diatas, kelas 0 (Tidak Churn) memiliki jumlah sampel yang jauh lebih banyak (83%) daripada kelas 1 (Churn) 17%."""

# Membuat visualisasi untuk melihat variable categori terhadap target(churn)
fig, ax = plt.subplots(1, 5, figsize=(35, 5))
for i, col in enumerate(cat_features):
    sns.countplot(x=col, hue='Churn', data=df, ax=ax[i])
    ax[i].set_xlabel(col, fontsize=12)
    ax[i].set_ylabel('Count', fontsize=12)
    ax[i].tick_params(axis='x', labelsize=10)
    ax[i].legend(['Not Churn', 'Churn'], loc='upper center', fontsize=12)
    ax[i].tick_params(axis='y', labelsize=12)

"""Dapat diketahui bahwa:
* Pada kolom `PrefferedLoginDevice` Pelanggan yang menggunakan *Mobile Phone* lebih banyak melakukan *churn* daripada menggunakan komputer
* Pada kolom `PreferredPaymentMode` Pelanggan yang menggunakan *Debit Card* atau *Credit Card* lebih banyak melakukan *churn*
* Pada kolom `Gender` dapat dilihat bahwa pelanggan laki-laki cenderung melakukan *churn* daripada perempuan
* Pada kolom `PreferedOrderCat` dapat dilihat bahwa kategori *Fashion* memiliki jumlah pelanggan yang lebih sedikit, namun memiliki tingkat churn yang relatif lebih tinggi, dengan 39 pelanggan yang melakukan churn dari total 165 pelanggan
* Pada kolom `MaritalStatus` dapat dilihat bahwa pelanggan dengan status pernikahan "Single" cenderung lebih mungkin untuk melakukan churn dibandingkan dengan pelanggan dalam status pernikahan lainnya,

# Data Preparation

## Splitting
"""

# Define dependent variable as target / label of prediction
y = df['Churn']

# Define independent variable as feature
x = df.drop(columns = ['Churn'], axis=1)

# Split Dataset for Training (80%) and Testing (20%)
x_train, x_test, y_train, y_test = train_test_split(
    x,
    y,
    test_size=0.2,
    random_state=1,
    stratify=y
) # 0.2 is 20% of the data as a tester  |  1 just for random | stratify for sampe proportion y

"""Tujuan proses splitting: <br>
* Pemisahan dataset menjadi data pelatihan (training) dan data pengujian (testing) adalah langkah penting dalam pengembangan model prediktif. Ini membantu dalam mengukur seberapa baik model yang dihasilkan akan berkinerja pada data yang belum pernah dilihat sebelumnya.
* Pembagian data menjadi 80% untuk pelatihan dan 20% untuk pengujian adalah pilihan yang umum dan dapat membantu memastikan bahwa model yang dikembangkan memiliki pelatihan yang memadai dan juga diuji dengan baik pada data yang independen
* Random State ini digunakan untuk memastikan pembagian data menjadi pelatihan dan pengujian tetap konsisten setiap kode dijalankan.
* Stratify ini juga memastikan bahwa pembagian data tetap menjaga proporsi kelas target untuk mencegah bias dalam pengujian

## Encoding

Pada langkah ini, akan dilakukan encoding (mengubah categorical menjadi numeric) untuk fitur kategori, salah satu teknik yang umum dilakukan adalah teknik one-hot-encoding<br>
Variabel Kategori:
* PreferredLoginDevice,
* PreferredPaymentMode,
* Gender,
* PreferedOrderCat,
* MaritalStatus <br>


 One Hot Encoding mengubah setiap kategori menjadi variabel biner (0 atau 1), yang memungkinkan model untuk menganggap setiap kategori secara terpisah tanpa memberikan urutan atau hierarki yang salah.
"""

# One Hot Encoding
one_hot_var = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus']
one_hot_encoder = OneHotEncoder()

# Combine preprocess in one table
preprocess= ColumnTransformer([
                                ('one_hot', one_hot_encoder, one_hot_var),
                                ], remainder='passthrough')

x_train

# Set the option to show all columns
pd.set_option('display.max_columns', None)

# Combine spliting data into preprocessing usin .fit_transform
x_train_prep = preprocess.fit_transform(x_train)
x_test_prep = preprocess.transform(x_test)

x_train_prep = pd.DataFrame(x_train_prep)
x_test_prep = pd.DataFrame(x_test_prep)

x_train_prep

# Getting Feature Names from one_hot_encoding
name_one_hot = list(preprocess.transformers_[0][1].get_feature_names_out())

# Adjust sequence in naming based on ColumnTransformer
feature_names = name_one_hot + ['Tenure', 'CityTier', 'WarehouseToHome', 'HourSpendOnApp', 'NumberOfDeviceRegistered', 'SatisfactionScore', 'NumberOfAddress', 'Complain', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder', 'CashbackAmount']
feature_names

# Input feature_names into x_train/test_preprocess
x_train_prep.columns = feature_names
x_test_prep.columns = feature_names

x_train_prep

"""# Modeling

Telah diketahui bahwa tujuan dari model adalah untuk memprediksi customer churn, maka model yang digunakan adalah klasifikasi. Dan hal-hal yang perlu dipertimbangkan adalah data bersifat imbalance dan range valuenya sangat beragam. maka data perlu dipertimbangkan untuk dilakukan perlakuan data imbalance dan scale.

Model yang dicoba:
- *K-Nearest Neighbors* (KNN)
- *Random Forest Classifier*
- *AdaBoost Classifier*
"""

# Dataframe untuk analisis model
models = pd.DataFrame(index=['train_f1_score', 'test_f1_score'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""### KNN

*K-Nearest Neighbors* (KNN) <br>
Mengukur jarak antara instance baru dan instance tetangganya untuk menentukan kelasnya. <br>
Kekurangan: <br>
*  Sensitif terhadap pemilihan parameter k (jumlah tetangga yang akan digunakan).
*  Mahal secara komputasi saat digunakan pada dataset besar.
*  Tidak memahami hubungan antar fitur atau pentingnya fitur tertentu. <br>

Kelebihan: <br>
*  Sederhana dan mudah dimengerti.
*  Cocok untuk dataset dengan pola kelas yang tidak teratur.
"""

# Menggunakan algoritma KNN
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(x_train_prep, y_train)

models.loc['train_f1_score','knn'] = f1_score(y_pred = knn.predict(x_train_prep), y_true=y_train)

"""```
# This is formatted as code
```

Parameters:
- *n_neighbors* merupakan parameter yang mengacu pada jumlah tetangga terdekat yang akan digunakan oleh model untuk membuat prediksi atau melakukan klasifikasi pada data baru. Dengan kata lain, *n_neighbors* menentukan berapa banyak titik data terdekat yang akan dipertimbangkan oleh model ketika mencoba memprediksi label atau nilai target dari data yang belum pernah dilihat sebelumnya.

### Random Forest

*Random Forest Classifier*
Menciptakan "hutan" pohon keputusan, dimana setiap pohon secara independen memprediksi kelas instance, Prediksi akhir dibuat dengan voting mayoritas. <br>
Kekurangan:
* Sulit untuk diinterpretasi dibandingkan dengan pohon keputusan tunggal.
* Memerlukan lebih banyak sumber daya komputasi daripada pohon keputusan tunggal.
  
Kelebihan: <br>
* Mengurangi overfitting dengan menggabungkan hasil dari banyak pohon keputusan.
* Mampu menangani dataset dengan banyak fitur dan kelas.
* Cocok untuk masalah klasifikasi dan regresi.
"""

# Menggunakan algoritma RandomForest
rf = RandomForestClassifier(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
rf.fit(x_train_prep, y_train)

models.loc['train_f1_score','RandomForest'] = f1_score(y_pred=rf.predict(x_train_prep), y_true=y_train)

"""Parameters:
- *n_estimators* merupakan jumlah pohon keputusan yang akan digunakan dalam *ensemble* Random Forest. Semakin banyak pohon yang digunakan, semakin kuat kemampuan generalisasi model, tapi juga semakin besar komputeasi yang diperlukan.
- *max_depth* mengontrol kedalaman maksimum setiap pohon keputusan dalam ensemble. Ini merupakan cara untuk mencegah overfitting. Semakin dalam pohon, semakin rumit modelnya yang menyebabkan overfitting pada data pelatihan.
- *random_state* merupakan seed untuk generator angka acak. Pengaturan *random_state* memastikan bahwa pengacakan data yang digunakan dalam pembuatan setiap pohon tetap konsisten, sehingga Anda dapat mendapatkan hasil yang dapat direproduksi.
- *n_jobs* untuk mengontrol berapa banyak pekerjaan paralel yang akan dijalankan saat melatih model.

### AdaBoost Classifier

*AdaBoost Classifier* <br>
Secara iteratif melatih pengklasifikasi yang lemah pada subset data yang berbeda, memberi bobot lebih pada instance yang salah klasifikasi di setiap iterasi. Prediksi akhir dibuat dengan menggabungkan prediksi dari semua pengklarifikasi dengan bobot telah diberikan untuk kinerja yang lebih kuat <br>
Kekurangan: <br>
 * Rentan terhadap noise dan outlier dalam data pelatihan.
 * Kinerja dapat terpengaruh jika pengklasifikasi lemah tidak baik atau terlalu kompleks.
 * Memerlukan penyetelan parameter yang hati-hati.

Kelebihan: <br>
 * Meningkatkan kinerja model dengan menggabungkan pengklasifikasi lemah.
 * Dapat digunakan dengan berbagai algoritma pengklasifikasi lemah.
 * Cenderung mengurangi overfitting.
"""

# Menggunakan algoritma Boosting Algorithm = AdaBoost
boosting = AdaBoostClassifier(learning_rate=0.05, random_state=55)
boosting.fit(x_train_prep, y_train)

models.loc['train_f1_score','Boosting'] = f1_score(y_pred=boosting.predict(x_train_prep), y_true=y_train)

"""Parameter:
- *learning_rate* merupakan hyperparameter yang mengontrol seberapa besar langkah yang diambil dalam mengoreksi kesalahan model pada setiap iterasi atau langkah dalam algoritma boosting. Semakin kecil *learning_rate* semakin lambat model akan belajar, tapi ini jug abisa membantu mencegah overfitting. Begitu sebaliknya.
- *random_state* merupakan seed untuk generator angka acak. Pengaturan *random_state* memastikan bahwa pengacakan data yang digunakan dalam pembuatan setiap pohon tetap konsisten, sehingga Anda dapat mendapatkan hasil yang dapat direproduksi.

# Evaluation
"""

# Create a DataFrame to store F1 scores for training and testing
f1_scores_df = pd.DataFrame(index=['train_f1_score', 'test_f1_score'], columns=['KNN', 'RandomForest', 'Boosting'])

# Create a dictionary for each algorithm used
model_dict = {'KNN': knn, 'RandomForest': rf, 'Boosting': boosting}

# Calculate F1 scores on the training and testing data for each algorithm
for model_name, model in model_dict.items():
    f1_scores_df.loc['train_f1_score', model_name] = f1_score(y_true=y_train, y_pred=model.predict(x_train_prep))
    f1_scores_df.loc['test_f1_score', model_name] = f1_score(y_true=y_test, y_pred=model.predict(x_test_prep))

# Display the F1 scores DataFrame
f1_scores_df

# Visualisasi dari tabel diatas
fig, ax = plt.subplots()
f1_scores_df.plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dari tabel tersebut, dapat menarik beberapa insight yaitu:
- pada model KNN menunjukkan kinerja yang lebih baik pada data pelatihan daripada pada data uji, menunjukkan adanya overfitting.
- pada model Random Forest memiliki kinerja yang hampir sempurna pada data pelatihan, yang mungkin menunjukkan overfitting, tetapi kinerja yang sangat baik pada data uji, menunjukkan kemampuan generalisasi yang baik.
- pada model Boosting menunjukkan kinerja yang rendah pada kedua data pelatihan dan data uji, menunjukkan kesulitan dalam menyesuaikan diri dengan data pelatihan dan kurangnya kemampuan generalisasi.

Random Forest mungkin merupakan pilihan terbaik karena memiliki keseimbangan antara kinerja pada data pelatihan dan data uji, meskipun penting untuk melanjutkan evaluasi dan tuning untuk memastikan hasil yang optimal.

F1 Score merupakan metrik evaluasi yang digunakan untuk mengukur kinerja model klasifikasi dengan cara memberikan keseimbangan antara Recall dan Presisi dan berguna dalam situasi dimana kita ingin meminimalkan false positive dan false negative secara seimbang. <br>

Semakin tinggi nilai F1 Score, semakin baik performa model dalam klasifikasi dengan nilai maksimum 1 yang menunjukkan performa sempurna
"""